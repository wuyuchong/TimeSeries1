---
title: "Homework Assignment 1"
author:
- Uppsala University
- Department of Statistics
- "Course: Time Series, Fall 2019"
- "Author: Claes Kock, Yuchong Wu, Emma Gunnarsson"
- "Date: 25/11/2019"
header-includes:
   - \usepackage{array}
   - \usepackage{booktabs}
   - \usepackage{diagbox}
   - \usepackage{textcomp}
   - \usepackage{float}
output:
  pdf_document:
    number_sections: yes
---

\newpage

\setcounter{tocdepth}{2}

\tableofcontents

\newpage

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(reshape2)
library(ggplot2)
library(moments)
```



```{r}
set.seed(931229)

# Parameter values and settings
NumObsSim     = 5000 # Simulated  size 
numObsToPlot  = 500  
ACFLagstoPlot = 20
sigma2        = 1    # variance of noise term
```


\newpage

# Introduction

Each stochastic process implies a theoretical structure of the autocorrelation function (ACF). A particular dataset is a *realization* of some specific stochastic process. Hence, by analyzing at a  autocorrelation function (PACF), one can approximate the underlying stochastic process, and find the proper model to estimate it, thereby being able to make forecasts. However, to be capable of matching a  ACF with a theoretical one, one must first know the properties of the theoretical stochastic process and thereby it's ACF. Hence, we need to have a catalogue of theoretical ACFs. The purpose of this paper is to review some MA(q), AR(p) and ARMA(p,q) processes, to create such a catalogue.

Each section in the paper starts with a general discussion about the process in question, followed by a simulation part where we generate a  based on the process in question. In that way, we can study the correlogram of the process. One important aspect of the discussion is whether a particular process is covariance stationary or not since this has major implications for how to work with the process. The  size in the simulations is set to 5000 (if nothing else is stated), but the plots only runs up to 500. 

The paper is structured as follows. In section 2, the concept of covariance stationary is defined, which will be a point of reference in most of the following discussions. Section 3 and 4 discuss the MA(1) and MA(2) processes respectively, and section 5 and 6 discuss the AR(1) and AR(2). Section 7 covers the ARMA() process, and section 8 concludes. Information about the derivations of expected values can be found in Appendix A. The actual models we study can be found in Appendix B.


\newpage


# The Concept of stationarity

A covariance stationary process is a process where the statistical properties do not change over time. This is of importance when we are trying to estimate a process. More specifically, covariance stationarity imposes three requirements:

1.	$E(Y_t)$ constant over time
2.	$Var(Y_t)$ constant over time
3.	The autocovariance depends only on the distance between two observations, and not on where in time the observations are found

\newpage

# Study for MA(1)

A moving average process of order 1 -an MA(1) proccess- indicates a process where the output variable $Y_t$ depends linearly on current white noise as well as white noise in the previous period. More specifically, we have the model:

MA(1):     $$Y_t = e_t - \theta e_{t-1}$$
where $e_t \sim NID(0, 1)$

$Y_t$ is an output variable, $\theta$ the parameter of the process, and $e_1$ denotes the white noise. $e_t$ is identically and independently distributed with mean 0 and a constant variance. Since it is independent, the autocovariance is 0 by definition, making any kind of predictions of $e_t$ impossible. This has indications for the properties of $Y_t$, which are discussed further in Appendix A. Here however, it is sufficient just to present the resulting formulas for the statistical properties of $Y_t$:

$$E(Y_t)=0$$
$$\gamma_0 = V(Y_t) = (1+\theta) * \sigma^2$$
$$\gamma_1 = Cov(Y_t, Y_{t-1})=\theta*\sigma^2$$
$$\gamma_2 = Cov(Y_t, Y_{t-2})= 0$$
$$\rho_1 = \gamma_1/\gamma_0 = \theta/(\theta^2+1)$$
$$\rho_2 = \gamma_2/\gamma_0 = 0$$

To have a covariance stationary MA(1) process, all three requirements for covariance stationarity must be fulfilled. Just by looking at the formula for the expected value and variance of $Y_t$, one understands that the first and second requirements indeed are fulfilled. Neither one of the expressions is related to time. Moreover, to have a covariance stationary process, the autocovariances must be independent of where in the process the observations are found, and only depend on the distance between those observations. This is indeed the case, which is shown in Appendix A. As you see above, the first autocorrelation (using the lag 1) is a function of $\theta$ and $\sigma^2$, and the other covariances are simply 0. Hence, they do not depend on time, but only on the distance.

We can hence conclude that an MA(1) process should be stationary no matter what the parameter is, as long as it is finite. Moreover, the first autocovariance is (positively) related to the MA(1) parameter, while the following ones are not. This has, of course, implications for the autocorrelations, since the autocorrelation is a function of the covariance. We expect a spike in the ACF at k=1, and 0 thereafter. Just by looking at the model one realizes that this is the case. $Y_t$ is related to the white noise only one period back. Hence, the interdependence between $Y_t$ and $e_{t-k}, k>1$ must be nonexistent.

The plot of PACF shows the partial autocorrelation function, which shows the autocorrelation using the lag k while controlling for the other k-1 lags. Hence, it is the marginal correlation of $Y_t$ and $Y_{t-k}$. We expect a geometrically decreasing PACF for an MA(1) process. Note here that the blue dotted lines in the correlagrams are the confidence intervals at the 95%-level. Hence, any spike reaching beyond a blue line is statistically different from 0 with 95% confidence.  

To get an even deeper understanding of how the MA(1) process behaves, we will now turn to a simulation part where the parameter value will be varied. All simulation plots can be found in Appendix B. We move through our tables from top to bottom, following the values of their parameters. We will be looking at each table at a time, with the following structure:

Table X(variable = y,z,x), meaning the first model has a variable value of y, second model z, etc.

Table 1($\theta=-1,-0.45,0$)
When $\theta=-1$, $Y_t$ depends negatively on $e_{t-1}$. The  variance of the plot looks constant, which aligns with theory since the true variance is $1*(1+(-1)^2)= 2$. Moreover, there is no trend in our data, but rather a choppy spread centered around 0. The reason for this is the nature of $e_t$ as an independent, identically distributed random variable. When turning to the correlograms, they look as expected. The  ACF has a spike statistically different from 0 at k=1 where the true 
$\rho = (-1/((-1)^2+1)= -0.5)$, and is 0 otherwise. The  PACF looks geometrically decreasing, which is what we expect.

When $\theta$ =-0.45, the model resembles the previous one, to a large degree. The major difference is that the variance of $Y_t$ now is smaller. One can see how the time series only moves between -3 and 3, instead of between -4 and 4 as in the previous case (note the different scales of the plots!) This makes sense since the true variance in this case only is 1.2025. ACF looks like what we expect (true value -0.372 when k=1) and the PACF seems to be declining as we expect as well.

When $\theta=0$, the moving average process collapses into just being white noise. Hence, we now have the model $Y_t$ = $e_t$, with mean 0 and variance $\sigma^2$. Since the autocovariance is zero, so no matter what k is, $\rho=0$ and hence we see no spikes whatsoever in the  ACF. $Y_t$ is **independently** distributed, which makes it not just covariance stationary but strict stationary.

Table 2($\theta=0.45,1,2$)
Here, $\theta$ is set to be **positive**, beginning at 0.45. Still, there is no evident trend in our data, but rather a choppy spread centered around 0. As in the previous cases, the variance of $Y_t$ looks stable in the time series plot, being indicative of the true variance which in this case is constant at 1.2025. Note that the variance of $Y_t$ thus does not depend on the sign of $\theta$, since it equals the variance in the second row. Moreover, the  value of ACF when k=1 is of the same magnitude as in row 2, but it is now positive. This also makes perfect sense, since $Y_t$ is *positively* related to $e_t$ in this case.

When increasing $\theta$ further to 1, the true variance increases to 2 (as in Table 1, $\theta=-1$). Moreover, the value of the ACF when k=1 is of the same magnitude as then, but now it is positive instead of negative. ACF and PACF both look correct. The plot is quite choppy, with a big ravine in the middle.

In the last row in Table 2, $\theta$ is increased to 2. This model is very similar to the previous one. The  ACF and PACF both resemble the previous ones. Hence, we have *doubled* $\theta$, but we see no great changes in the simulation. 

In summary, all simulations of the MA(1) process, using different parameter values, generate stationary s of data. All realizations presented here have, by visual inspection, constant means and variances. Moreover, the autocovariances do not depend on time but only on the time lag. Finally, in all cases except for when $\theta$ is 0, we have some interdependence, making the data covariance stationary instead of strict stationary.

\newpage

# Study for MA(2)

An MA(2) process includes not only white noise in the current and previous periods like the MA(1) process, but it also includes white noise *two* time periods back. More specifically, we have the model:

MA(2):     $$Y_t = e_t - \theta_1 e_{t-1} - \theta_2 e_{t-2}$$
where $$e_t \sim NID(0, 1)$$

As before, $Y_t$ is an output variable, and $e_t$ denotes the white noise. The $\theta_1$ parameter describes how the previous period is related to $Y_t$, and $\theta_2$ is the parameter describing how $e$ two periods back is related to $Y_t$. Moreover, $e_t$ is identically and independently distributed with mean 0 and a constant variance just as in the MA(1) process. Below, the statistical properties of an MA(2) process are presented. To see derivations and further discussion, please see Appendix A.

$$E(Y_t)=0$$
$$\gamma_0 = V(Y_t) = (1+\theta_1^2+\theta_2^2) * \sigma^2$$
$$\gamma_1 = Cov(Y_t, Y_{t-1})=\theta_1*\sigma^2 + \theta_1*\theta_2*\sigma^2$$
$$\gamma_2 = Cov(Y_t, Y_{t-2})= \theta_2*\sigma^2$$
$$\rho_1  = \theta_1*(\theta_2+1) /(\theta_1^2 + \theta_2^2+1)$$
$$\rho_2 = \theta_2 / (\theta_2^2 + theta_1^2+1)$$

In conclusion, an MA(2) process should be stationary no matter what the parameters are, as long as they are finite. When it comes to the ACF, we expect spikes at k=1 and k=2, but not thereafter, since the autocovariances for k>2 are equal to 0. Also, we expect a geometrically decreasing PACF.

In the following, we will discuss simulations of the MA(2) process using different combinations of $\theta_1$ and $\theta_2$. The section follows this structure: Table X($\theta_{2}$). $\theta_{1}$ is set to the values, -0.8, 0 and 0.8 in that order.

Table 3($\theta_{2}=0$)
In Table 3,  since $\theta_{2}=0$, the MA(2) process essentially collapses into  MA(1) processes or simply white noise, depending on what $\theta_1$ is.

In the first cell $\theta_1=-0.8$, creating an MA(1) process with the parameter value -0.8. The fact that this is an MA(1) process is evident in the , since the  ACF only has one spike, at k=1. Moreover, the PACF looks similar to the one in the MA(1) processes where $\theta$ was set to negative values in Table 1.

When $\theta_1=0$, we once again get a white noise process. For a further discussion on white noise, please review the section discussing the MA(1) process where $\theta=0$ in Table 1. We can see similarities in ACF and PACF with this process, as we have no significant spikes, and a non-geometrical change in PACF.

When $\theta_{1}=0.8$, we get an MA(1) process with the parameter value 0.8. The  ACF only has one spike, at k=1, and the PACF looks similar to the ones in the MA(1) processes where $\theta$ was set to 1 and 2 respectively. The time series looks fairly centered, however, it is somewhat spread in some places.

Table 4($\theta_{2}=0.7$)
In Table 4, $\theta_2=0.7$, creating a full MA(2) process, with terms for the white noise in the current and past white noise, as well as the white noise two periods back.

When $\theta_1=-0.8$ and $\theta_2=0.7$, $Y_t$ is positively related to $e_t$ two periods back, and negatively related to $e_t$ in the previous period.  The  variance looks constant in the time series plot, which is according to theory since the true variance is constant at $1*(1+(-0.8)^2+0.7^2)= 1.4964$. Moreover, there is no trend in the data, but rather a choppy spread constantly around 0, just as in the MA(1) cases.  The  ACF now has two spikes, at k=1 and k=2. The first spike is negative (true value -0.638), indicating the negative relationship between $Y_t$ and $e_{t-1}$. The second spike is on the other hand positive (true value 0.230), since $e_{t-2}$ is positively related to $Y_t$.

When $\theta{_1}=0$, and $\theta{_2}=0.7$, the model becomes:

MA(2):    $$Y_t = 0.7*e_{t-2}  + e_t$$
where $e_t \sim NID(0, 1)$

We see only one spike in the  ACF, at k=2, which aligns with theory. Since $\theta_1=0$, the true first autocorrelation becomes 0, and the second becomes -in this case- 0.329. The  PACF is still geometrically decreasing. It does look like the  estimation is a bit bigger, but still pretty close.

When $\theta{_1}=0.8$, and $\theta{_2}=0.7$, $Y_t$ is positively related to e both one and two periods back. The  variance looks constant in the time series plot, which aligns with theory; a constant true variance at 2.49. The  ACF now has two spikes, at k=1 and k=2, and naturally, both spikes are positive. The true values of the ACF are 0.638 (k=1) and 0.329 (k=2).

Table 5($\theta_{2}=1$)
In Table 5,  $\theta_2=1$. Compared to before, when $\theta{_1}=-0.8$, and $\theta{_2}=1$, $Y_t$ is negatively related to $e$ one period back and positively related to e two periods back. The variance looks constant in the time series plot, which makes sense since the true variance is constant at 2.64. The  ACF looks like a good estimation of the true autocorrelations, as in the previous cases. The true value of the ACF when k=1 is -0.606, and when k=2 it is 0.379. The PACF is geometrically decreasing, as expected.

When $\theta{_1}=0$. and $\theta_2=1$, we get the model
MA(2):    $$Y_t = e_{t-2}  + e_t$$
where $e_t \sim NID(0, 1)$

As was the case in Table 4 when $\theta{_1}=0$, there is only one spike in the  ACF, at k=2. The  PACF is geometrically decreasing, also as expected.

When $\theta{_1}=0.8$, and $\theta_2=1$ the process is in most aspects similar to when $\theta_2$ is somewhat smaller.

\newpage

# Study for AR(1)


An autoregressive process of order 1, an AR(1), is a **recursive* *process, where $Y_t$ depends on itself one period back, as well as some white noise $e_t$. More specifically, we have the model:

$$Y_{t}=\phi_{1} Y_{t-1}+e_{t}$$
Where we assume $\operatorname{Cov}\left(Y_{t}, e_{t}\right)=0$, and where $e_{t} \sim iid(0,1)$

$\phi_1$ describes how $Y_t$ depends on $Y_{t-1}$, and as before, $e_t$ is identically and independently distributed with mean 0 and a constant variance. When having an AR process, it is impossible to calculate statistical properties such as expected value, variance and autocovariances without *assuming* stationarity. Thus, after the derivations have been made, one must check whether the assumptions actually are fulfilled in a particular AR(1). The full derivations are found in Appendix A, where the reasons for the neccessity of a stationarity assumption also are presented. Here, it suffices to just present the resulting formulas:

$$E(Y_t)=0$$ 
$$\gamma_0=\sigma^2/(1-\phi^2)$$
$$\gamma_1= \phi*\sigma^2 / (1-\phi^2)$$
$$\gamma_2= \phi^2*\sigma^2 / (1-\phi^2)$$
$$\rho_1= \phi \\ \rho_2= \phi^2$$

Once again note that these formulas are derived under the *assumption* of stationarity. Consequently, they are not valid if the assumption of stationarity is violated.

Unlike as in the MA() processes, the ACF (the $\rho$:s) in an AR(1) will not drop to 0 when k>1. The reason is the recursiveness of the AR(1) process. $Y_t$ depends on it's past values, which in turn depends on *it's* past values, etc. Hence, the relationship between $Y_t$ and any $Y_{t-k}$ can be decsribed by some specific function of $\phi_1$ To illustrate, we can subtsitute $Y_{t-1}$ in the model above:

AR(1): $$Y_t = \phi_1(\phi_1Y_{t-2}+e_{t-1}) + e_t$$
$$Y_t = \phi_1^2(\phi_1Y_{t-3}+e_{t-2})+\phi_1e_{t-1} + e_t$$
$$Y_t = \phi_1^3(\phi_1Y_{t-4}+e_{t-3})+\phi_1e_{t-2}+\phi_1e_{t-1} + e_t.$$

It becomes evident that that $Y_{t-2}$ is related to $Y_t$ by $\phi_1^2$, $Y_{t-3}$ by $\phi_1^3$ etc. 

Any $\rho_k$ will therefore also depend on $\phi_1$, more specifically $\rho_k=\phi_1^k$. Hence, the ACF will *not* be 0 for k>1. Thus, as long as $\phi_1<1$, we will expect a geometric decrease in the ACF as k increases.

Also, we expect the PACF to have only one significant spike, at k=1, since this is the case for AR(1) processes.

To check for stationarity in AR(1) processes, one studies the *characteristic equation*, which is defined as $(1-\phi_1X)=0$, generating the *root* $X=1/\phi_1$. This root is to be evaluated concerning the unit circle. If the roots in absolute value is smaller than 1, the process in question is nonstationary. Of course, this is equivalent to requiring the absolute value of $\phi_1$ to be smaller than 1. In summary, one must study a specific AR(1) process with a specific parameter value more in detail, to see whether it is stationary or not.

This will be done in the following. We will discuss in total 7 different AR(1) processes, with different values on $\phi_1$, and for each process, comments on stationarity will be made.

Table 6
In table 6 in Appendix B, we have the values $\phi_1=-0.1$,$\phi_1=-0.95$, and $\phi_1=-0.75$. As we can see, we only have negative values for these cells.
When $\phi_1=-0.1$, we can immediately see how $Y_t$ is centered around zero. Also, the variance seems to increase somewhat over time, which could indicate that this is not a covariance stationary process. However, the absolute value of $\phi$ is obviously smaller than 1. Hence, we do have a covariance stationary process. This makes the formulas for the statistical properties above valid.

The  ACF seems to oscillate between positive and negative values, reflecting the fact that $Y_t$ is negatively related to $Y_{t-1}$ in this process. It's hard to determine whether the ACF is declining or not, indicating a high interdependency. The true value of the ACF when k=1 is -0.1 and when k=2 it is $(-0.1)^2=0.01$. Furthermore, the  PACF only has one significant value and decreases very fast, which is to be expected for an AR(1) process. 

When $\phi_1=-0.95$ we have a rather choppy time series plot. The variance varies greatly from period to period, and $Y_t$ looks centered around zero, which would indicate nonstationarity. However $\phi_1$ is still smaller than 1, prooving the process to be covariance stationarity.

The  ACF still oscillates and seems to be declining the way we expect it to as k increases. The true value of the ACF is -0.95 (k=1) and 0.9025 (k=2). The  PACF looks like expected with only one significant value.

When $\phi_1=-0.75$, we still have an uneven, choppy time series plot. It seems to be more centered around zero overall, with a reduced variance compared to the second cell - the values only stretch between -4 and 4, instead of -6 and 6. We can also see that the  ACF declines much faster than before. The  PACF looks as expected.


Table 7
In table 6, we have the values $\phi_1=-0$,$\phi_1=0.75$,$\phi_1=-0.95$ and $\phi_1=1$. For this table, all values are positive.
When $\phi_1=-0$, the process just becomes white noise; we have no parameters but only the error term. For a closer discussion on white noise, please review the discussion for the MA(1) process where $\theta=0$, in Table 1.

For $\phi_1=0.75$, the plot looks centered around zero, but it looks rather uneven and choppy in its variation, the points seem to be spread further apart. It is stationary since $\phi_1<0.75$. The  ACF and PACF looks as expected, with a fast decline of the ACF.

The third cell $\phi_1=0.95$ looks even more uneven and spread out. The plot is uneven, going from 5 to -10. We seem to be looking at an increase in choppiness the more we go towards 1 in $\phi_1$. It looks like the plot is still focused on zero, but there is a lot more randomness in how far away from zero the values stretch. The  ACF decline has slowed considerably compared to the previous cell. The  PACF looks normal.

For the fourth and final cell, $\phi_1=1$, creating a *random walk* process. Since $\phi_1=1$, the process is not stationary.
In the time series plot, we can see any pretense of stationarity evaporating, as the plot has a slope. Thus, it is no longer centered around zero. It still is choppy and even more uneven than before, as the entire plot is moving downwards. The  ACF at the top of Table 6 and this one look similar, if we were to disregard the oscillating ACF of $\phi_1=-0.1$ and put all the values as positive. We can thus say that ACF starts declining slower when $\phi_1$ becomes larger. PACF remains the way we expect it to be, with only one spike.


\newpage

# Study for AR(2)

An autoregressive process of order 2, an AR(2), is a process where $Y_t$ depends on $Y_{t-1}$ as well as $Y_{t-2}$, and some white noise $e_t$.The process is *recursive*; $Y_t$ depends on it's own past values. More specifically, we have the model:

AR(2):     $$Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + e_t$$
where $e_t \sim NID(0, 1)$, and  $Cov(Y_{t-1}, e_t)=0$ by assumption. 

As the AR(1), the AR(2) process is not always stationary; it depends on the parameters of the process. The statistical properties stated below are derived under the *assumption* of stationarity, just as in the AR(1) case. The full derivations are found in Appendix A, and only the resulting formulas are presented here; 

Statistical properties:
$$E(Y_t)=0 \\ \gamma_0=\ \\ \gamma_1 = \\ \gamma_2 =  \\ \rho_{1}=\phi_{1}/(1-\phi_{2})\\ \rho_{2}=\phi_{1}^2/(1-\phi_{2})+\phi_{2}$$

As in the AR(1) case, the ACF (the $\rho$:s) in an AR(2) will not drop to 0 when k is greater than the order of the process (in this case when k>2). The reason is again the recursiveness of the AR process. To illustrate also in the AR(2) case, we can subtsitute $Y_{t-1}$ and $Y_{t-2}$ in the model above:

AR(2): $Y_t = \phi_1(\phi_1Y_{t-2}+\phi_2Y_{t-3}+e_{t-1}) + \phi_2(\phi1_1Y_{t-3}+\phi_2Y_{t-4}+e_{t-2}) + e_t$

No need to mention that this substitution can go on forever so that $Y_t$ depends on all its past values.  It is harder to see the exact relationship here between $Y_t$ and any $Y_{t-k}$, making it harder to establish $\rho_k$, but what we at least know is that any $\rho_k$ will depend on $\phi_1$ and $\phi_2$ in some way. 

To check for stationarity in AR(2) processes, one studies the characteristic equation, which is defined as $(1-\phi_1X-\phi_2X^2)=0$. Solving for X will, in this case, generate two X-values, two *roots* of the unit circle. If *either one* of these roots in absolute value is smaller than 1, the process in question is nonstationary. As in the AR(1) cases, one must study a specific AR(2) process, to see whether it is stationary or not.

This will be done in the following. We will discuss in total 9 different AR(2) processes, with different combinations of values on $\phi_1$ and $\phi_2$, and for each process, comments on stationarity will be made.

In Table 8 in Appendix B, $\phi_2$ is held constant at 0.1, while $\phi_1$ varies. In the following, each combination will be discussed.

When $\phi_1=-0.9$, the $Y_t$ is negatively related to $Y_{t-1}$, and positively related to $Y_{t-2}$. The time series plot is centered quite densely around the mean, which by inspection seems to be 0. Hence, the first requirement for covariance stationarity seems to be fulfilled; a constant mean. However, the variance is obviously not constant; it increases over time and in some kind of cyclic pattern. Hence, this AR(2) process does not seem to fulfill the second requirement, namely a constant variance. This can be formally checked by analyzing the characteristic equation, which in this case is: 
$$(1-(-0.9)X-0.1X^2)=0 \\ (1+0.9X-0.1X^2)=0$$
solving for X gives the roots $X_1=9.1$ and $X_2=-0.11$, and since the latter is smaller than the unit circle, the process is indeed nonstationary. This, in turn, invalidates the statistical properties stated above.

Turning to the  ACF and PACF; we see a large interdependence, and basically no decrease of the spikes in the ACF as k increases. At the first lag, the  autocorrelation is negative, since $Y_t$ is negatively related to itself one period back. Using the same logic the autocorrelation at the second lag is positive. Moreover, we see that the ACF does not drop to 0 when k becomes bigger than 2 as in the MA() processes. This is exactly what we expected. Also, no decrease as k increases is seen in the  ACF, indicating a large interdependence. According to the formulas for $\rho_1$ and $\rho_2$ above, the value of the ACF when k=1 is $(-0.9)/(1-0.1)=-1$ and when k=1 it is $(-0.9)^2/(1-0.1) + 0.1=1$. However, those formulas were derived under the assumption of stationarity, which is not the case in this particular process.

The  PACF shows two significant spikes at k=1 and k=2, and thereafter the spikes are indistinguishable from 0. 

When $\phi_1=0$ instead, we get the model:

AR(2):     $$Y_t = 0.1* Y_{t-2} + e_t$$
So that $Y_t$ only depends on Y two time periods back, as well as the white noise. When studying the time series plot and the correlograms, it actually reminds a little bit of a white noise process. Perhaps, the reason is that $\phi_2$ is pretty small, so that the relationship between $Y_t$ and $Y_{t-2}$ does not become that pronounced. (For a discussion about white noise, please review the discussion of the MA(1) process, where $\theta=0$, in Table 1.) However, a vague resemblance is not an exact match.

To evaluate stationarity we turn to the characteristic equation:
$$(1-0.1X^2)=0$$
Solving for X we get the roots $X±3.16$, placing us outside the unit circle. Thus, this process is stationary, and as a result, the stated formulas above are valid. Calculating the first autocorrelation, we get $0/(1+0.9)=0$, and the second one is $0/(1+0.9)+0.1=0.1$. Looking at the  ACF, it actually looks like a good estimation of the true value, but since it is so small it is naturally harder to make precise estimations. Therefore, it falls within the 95% confidence interval.

When $\phi_1=0.7$ (and $\phi_2=0.1$ as before), the time series plot become less centered around 0. Instead, it looks like it is drifting a bit, although still around 0. Also, it is hard to say anything about whether the variance is constant or not. Consequently, it is hard to discern evidence on stationarity from the plot. Instead, we turn to the characteristic equation:
$$(1-0.7X-0.1X^2)=0$$
Which generates the roots $X=1.22$ and $X=-8.22$, placing us outside the unit circle. Hence, this particular AR(2) process is stationary. The true mean is 0, and the variance is constant. Of course, the autocovariances also consequently only depends on k, and not on time, since this is the third requirement for stationarity.

The true first autocorrelation is 0.778, and the second is 0.644. (Note: This doesn't match the correlogram.)

In Table 9 in Appendix B, $\phi_2=0.2$, while $\phi_1$ is allowed to vary. In the following, each combination will be discussed.

When $\phi_1$ simultaneously was set to -0.9, the simulation program broke down using the  size 5000. Therefore, the plot shows a  of 500 instead. In this AR process, $Y_t$ depends negatively on Y one period back, and positively on Y two periods back. Perhaps, it can be thought of as a more "extreme" version of the AR(2) $\phi_2=0.1$, $\phi_1=-0.9$ in Table 8. The difference between the two processes is a doubling of $\phi_2$. The time series plot shows little variation in Y (note however the scale!) until very late in the process, where the variance literally explodes. At the original  size, the variance probably became so very large so that the program could not handle it anymore. So the variance seems to diverge into infinity, and consequently, this process is obviously not stationary. However, to formally analyze this, we take a look at the characteristic equation:
$$(1-(-0.9)X-0.2X^2)=0 \\(1+0.9X-0.2X^2)=0$$
Which generates the roots: $X_1=5.42$ and $X_2=-0.922$. Since $X_2$ is within the unit circle, the process is non-stationary. Moreover, the  ACF is geometrically decreasing. At k=1, the value of the ACF is negative, and at k=2 it is positive, reflecting the signs of $\phi_1$ and $\phi_2$ respectively. Thereafter, the  ACF is geometrically decreasing, reflecting the recursiveness of the process. The  PACF shows a large spike at k=1, and thereafter no significant spikes at all. 

When $\phi_1$ is set to 0, we get the model:
AR(2):     $$Y_t = 0.2* Y_{t-2} + e_t$$
so that $Y_t$  depends on itself only two periods back. In many aspects, it is very similar to the AR(2) process $\phi_1=0, \phi_2=0.1$ in Table 8. However here, we do see some significant spikes in correlogram. That is logic since some more interdependence should be expected when $\phi_2$ is doubled. To check for stationarity we set up the characteristic equation:
$$(1-0.2X^2)=0$$
Which generates the roots $X±2.23$. Since they are outside the unit circle, this process is stationary too.

When $\phi_1$ is set to 0.7, the time series plot shows clear signs of a cyclic variation, centered around a value smaller than 0.

The characteristic equation:
$$(1-0.7X-0.2X^2)=0$$
generates the roots $X_1=4.59$ and $X_2=-1.09$. Since they are both outside the unit circle, the process is stationary. The true first autocorrelation is $(0.7/(1-0.2)=0.875$ and the second is $0.7^2/(1-0.2)+0.2=0.813$. They seem to be properly estimated when looking at the  ACF graph. The spikes in the  ACF continue to decrease towards 0 as k increases. Moreover, there are 2 significant spikes in the  PACF. This process has many similarities with the AR(2) $\phi_1=0.7, \phi_2=0.1$ in Table 8. Here, however, ACF decreases at a greater speed. Also, the value of the  PACF is a lot larger, while it is smaller when k=2.


In Table 10 in Appendix B, $\phi_2=0.8$, while $\phi_1$ is allowed to vary. In the following, each combination of parameters will be discussed.

When $\phi_1=-0.9$ the simulation program broke down at the original  size. Therefore, we chose a  size of 500 instead. The process very much looks like a furthermore "extreme" case of the AR(2) $\phi_2=0.1$, $\phi_1=-0.9$ in Table 8, as well as the AR(2) $\phi_2=0.2$, $\phi_1=-0.9$ in Table 9. The variance increases towards infinity, and the process is obviously not stationary. Hence, as $\phi_1$ is held constant at -0.9, the processes become more and more "extreme" as $\phi_2$ increases.

When $\phi_1$ is set to 0, we once again (as in AR(2) $\phi_2=0.1 \phi_1=0$ in Table 8, or $\phi_2=0.2 \phi_1=0$ in Table 9) have an AR(2) model with only two terms:
AR(2):     $$Y_t = 0.8* Y_{t-2} + e_t$$

However, this process does not at all resemble white noise. We have a lot of significant spikes in the  ACF, and one significant spike in the  PACF. The interdependence is naturally larger here, since $\phi_2$ has been quadrupled compared to Table 9. 

To check for stationarity we set up the characteristic equation:
$$(1-0.8X^2)=0$$
Which generates the roots $X±1.118$. Since they are outside the unit circle, this process is stationary. One interesting note is that the bigger the $\phi_2$ has become (holding $\phi_1$ constant at 0), the closer have the roots come to unity. It is very plausible that if $\phi_2$ was increased further, the process would become nonstationary.

When $\phi_1=0.7$ in Table 10, the simulation program broke down at the original  size. Therefore, the  size was set at n=500 instead. In the time series plot, there seems to be little variation (not however the scale!) until the time periods just before 500, where it dramatically drops. Actually, this looks like an ARMA process.

We look for stationarity by analyzing the characteristic equation:
$$(1-0.7X-0.8X^2)$$
which gives the roots: $X_1=-1.64$ and $X_2=0.76$. Since the latter is within the unit circle, the process is -perhaps not surprisingly after inspecting the time series plot- nonstationary. The  ACF is decreasing as k increases, and the  PACF is showing one spike at k=1 but nothing thereafter.

\newpage

# Study for ARMA(1,1)

An Autoregressive Moving Average Model of degree (1,1) includes both the MA(1) process and the AR(1) process. More specifically, we have the model:

ARMA(1,1): $$Y_t = (\phi Y_{t-1} + e_t) - \theta e_{t-1}$$
Wich makes it evident that $Y_t$ depends both on itself in the past period, as well as white noise in the current and past periods.

$\phi$ describes how $Y_t$ depends on $Y_{t-1}$ (like the parameter of the AR(1) process), $\theta$ describes how $Y_t$ depends on $e_{t-1}$ (like the parameter of the MA(1) process), and as before, $e_t$ is identically and independently distributed with mean 0 and variance 1. As in the AR case, it is impossible to derive formulas for the statistical properties of the process, whithout *assuming* stationaity. Thus, after the derivations have been made, one must check whether the assumptions actually are fulfilled in a particular ARMA(1,1). The full derivations are found in Appendix A. Here, it suffices to just present the resulting formulas:

$$E(Y_t) = \frac{1}{1-\phi}$$

$$\gamma_0=\phi \frac{(\phi+\theta)(1+\phi \theta)}{1-\phi^{2}} \sigma^{2}+\sigma^{2}(1+\theta(\phi+\theta))$$
$$\gamma_{1} =\frac{(\phi+\theta)(1+\phi \theta)}{1-\phi^{2}} \sigma^{2}$$
$$\gamma_{2} =\phi\frac{(\phi+\theta)(1+\phi \theta)}{1-\phi^{2}} \sigma^{2}$$
$$\rho_{1} =\frac{(\phi+\theta)(1+\phi \theta)}{1+2\phi\theta+\theta^2}$$ 
$$\rho_2=\phi\frac{(\phi+\theta)(1+\phi \theta)}{1+2\phi\theta+\theta^2}$$

Since the formulas have been derived under the assumption of stationarity, they are not valid in a nonstationary process. Hence, it is important to know the status of stationarity in a specific process. 

An ARMA process contains both an MA and an AR part. Remember that the MA processes always are stationary, no matter what the parameter values $\theta_i$ are, while an AR process must be evaluated in each specific case. If the root(s) of the characteristic equation in absolute values is (are) smaller than 1, the process in question is stationary. Hence, when evaluating whether an ARMA process is stationary, we only use the AR part. Consequently, we use the same characteristic equation as in the AR(1) case: $(1-\phi_1X)=0$, generating the *root* $X=1/\phi_1$. Thus, in order to have stationarity, the absolute value of $\phi_1$ must be smaller than 1. 

In the following, we will look closer at some specific simulated ARMA(1,1) processes. We will discuss in total 6 different ones. We use the variables $\theta$ and $\phi$. In all cases, we expect both the ACF and PACF to be geometrically declining, which in a way illustrates the combo of an AR (geometrically decreasing ACF), and an MA (geometrically decreasing PACF).


Table 11
In Table 11, we have $\theta = -0.4$, while $\phi$ is allowed to vary. When, $\phi=-0.9$ in the first row, the time series plot is centered around 0, and the variance looks fairly constant. Since the absolute value of $\phi$ is smaller than 1, the process is stationary. Therefore, the formulas for the statistical properties above are valid. The true expected value is in fact 0.526, and the true variance is 9.89. The  ACF is oscillating and declining, although not that rapidly. The true first autocorrelation is according to the formula above -0.94, and the second autocorrelation is 0.85. The ACF actually to some extent ressembles the one in the AR(1) process where $\phi=-0.95$ in Table 6. The  PACF also declines, but faster. After three or four spikes, no values are significant anymore. Although not a perfect match, it ressembles the PACF for the MA(1) process where $\theta=-0.45$ in Table 1.

In the second row $\phi=0.8$, so that $\phi$ now is *positive*. The variance looks smaller, since the plot is only moving between -4 and 4. Moreover, the  ACF seems to decline faster, and does no longer oscillate. The true value of the ACF is 0.52 (k=1) and 0.41 (k=2). To some extent, it is similar to the ACF in the AR(1) processes in Table 7, where $\phi$ equals 0.74 and 0.95 The PACF has four significant spikes and drops thereafter to 0. Unlike the PACF in the previous row, all significant spikes are now positive. When comparing with the MA(1) process where $\theta_1=-0.45$ in Table 1, we see that both are decreasing geometrically. However the MA(1) process generates negative values on the PACF.

When $\phi = 0.9$ in the third row, we do see changes. However since the absolute value of $\phi$ still is less than 1, we know that the process is stationary. The ACF is declining towards zero pretty fast and does not oscillate. Not how it (perhaps vaguely) ressemble the ACF:s for the AR(1) where $\phi_1=0.95$ and when $\phi_1=0.75$ in Table 7. Indeed, it seems like this ACF falls somewhere inbetween the two. The PACF is very similar to the one in the previous case; geometrically declining with three or four significant spikes.

In Table 11, $\theta_1$ is not changed, but obviously the look of the PACF does change. Of course, this is because of the fact that the ARMA(1,1) process also encompasses the "AR(1)-part". We also see that when holding $\phi_1$ constant and instead changing $\theta_1$ (match the rows in Table 11 and 12), the ACF changes somewhat as well. Hence, the details of the ACF of course also depends on the "MA(1)-part" of the ARMA(1;1) process.


Table 12
In Table 12, $\theta = 0.4$ instead. The values of $\phi$ are varied in the same way as before.

When $\phi=-0.9$, the process looks somewhat similar to the first row of Table 11. The process is stationary, since the absolute value of $\phi$ is smaller than 1. The ACF is oscillating and declining, in a similar way as in the AR(1) where $\phi=-0.95$ in Table 6. The PACF has around four significant spikes, and is geometrically decreasing. In constrast to the PACF of the counterpart in Table 11, this one is oscillating.

When $\phi=0.8$, in the second row, we know that the process is still stationary, since $|\phi|$ is smaller than 1. It also *looks* stationary since the mean and variance seems to be constant. The ACF looks as expected. According to the formulas, the first autocorreltation is 0.88, and the second is 0.13. Moreover, it is somewhat similar to the AR(1) where $\phi_1$ is 0.70 to 0.95 in Table 7.

The time series plot in the third row, where $\phi=0.9$, behaves very similarly to its counterpart in Table 11. However now the PACF is oscillating.  When studying all PACF in Table 11 and 12, it seems like any ARMA(1,1) with a positive parameter value will get an oscillating PACF. When $\theta$ is negative, the PACF could be either positive or negative, but not oscillating. The  crude shape of the ACF seems on the other hand to depend only on the sign of $\phi$; an positive $\phi$ generates a positive ACF, and a negative $\phi$ gives an oscillating one.



\newpage

# Conclusion

We have studied five different main processes, to see how their realizations change depending on the parameter values of the process in question. From our models we have simulated plots which we have then analyzed. Most of the plots and tables have been within expectation. We have studied the processes of Moving Averages (MA) and AutoRegession (AR), as well as the combination of these models, the Autoregressive Moving Average process (ARMA). Since all of these processes in some way or another are dependent on a *white noise* process, white noise has also been an important part of our discussion. All models have been evaluated in relation to *covariance stationarity*, which is crucial to be able to estimate a model. We have derived the processes theoretically correct values, which we have then compared with simulated data time series plots and their correlograms. We found MA models to be always covariance stationary, no matter what the parameter(s) of the process is(are). AR models are on the other hand stationary if the absolute value of the root of the *characteristic equation* is bigger than 1. In the AR(1) case, that is equivalent to requiring the absolute value of $\phi$ to be smaller than 1. If $\phi=1$ we have a *Random Walk* process, which of course then is not stationary. Since it is the AR-part of the ARMA process that might make it nonstationary, one uses the same characteristic equation when evaluating the ARMA process. Thus, in an ARMA(1,1) process, the absolute value of $\phi$ is required to be smaller than 1 in order to have stationarity. Hence, we have looked on both stationary and nonstationary AR(1), AR(2) and ARMA(1,1) processes. When studying the particular ARMA(1,1) processes using different parameter values, one can indeed see many similarities with the MA(1) and the AR(1) processes. In summary, we have deepened our understanding of several different processes, and indeed created a small "catalog" of processes to be used in real life estimation in the future.



\newpage


# Appendix A


## Derivation for MA(1)

### Model

$$Y_{t}=e_{t}-\theta e_{t-1}$$

$$e_{t} \sim I I D\left(0, \sigma^{2}\right)$$

### Mean

$$\begin{array}{l}{E\left(Y_{t}\right)=E\left(e_{t}-\quad \theta e_{t-1}\right)} \\ {E\left(Y_{t}\right)=E\left(e_{t}\right)-E\left(\theta e_{t-1}\right)} \\ {E\left(Y_{t}\right)=E\left(e_{t}\right)-\theta E\left(e_{t-1}\right)} \\ {E\left(Y_{t}\right)=0 \quad-\theta \times 0} \\ {E\left(Y_{t}\right)=0}\end{array}$$

### Variance

$$\begin{array}{l}{\operatorname{Var}\left(Y_{t}\right)=V\left(Y_{t}\right)} \\ {\operatorname{Var}\left(Y_{t}\right)=V\left(e_{t}+-\theta e_{t-1}\right)} \\ {\operatorname{Var}\left(Y_{t}\right)=V\left(e_{t}\right)+V\left(-\theta e_{t-1}\right)+2 \operatorname{Cov}\left(e_{t},-\theta e_{t-1}\right)}\\ {\operatorname{Var}\left(Y_{t}\right)=V\left(e_{t}\right)+(-\theta)^{2} V\left(\quad e_{t-1}\right)+(-\theta) 2 \operatorname{Cov}\left(e_{t}, \quad e_{t-1}\right)} \\ {\operatorname{Var}\left(Y_{t}\right)=V\left(e_{t}\right)+\quad \theta^{2} \quad V\left(\quad e_{t-1}\right)+(-\theta) 2 \times 0} \\ {\operatorname{Var}\left(Y_{t}\right)=V\left(e_{t}\right)+\quad \theta^{2} \quad V\left(\quad e_{t-1}\right)}\\ {\operatorname{Var}\left(Y_{t}\right)=V\left(e_{t}\right)+\theta^{2} V\left(e_{t}\right)} \\ {\operatorname{Var}\left(Y_{t}\right)=\sigma^{2}+\theta^{2} \sigma^{2}} \\ {\operatorname{Var}\left(Y_{t}\right)=\sigma^{2}\left(1+\theta^{2}\right)}\end{array} $$

### First autocovariance

$$\begin{aligned} \operatorname{cov}\left(Y_{t}, Y_{t-1}\right)=& \operatorname{Cov}\left(\theta_{1} e_{t-1}+e_{t}, \quad \theta_{1} e_{t-2}+e_{t-1}\right) \\=& \operatorname{Cov}\left(\theta_{1} e_{t-1} \quad \quad, \quad \theta_{1} e_{t-2} \quad \quad \quad)\right.\\+& 
\operatorname{Cov}\left(\theta_{1} e_{t-1} \quad \quad, \quad \quad \quad \quad \quad e_{t-1})\right.\\+& 
\operatorname{Cov}\left(\quad \quad \quad \quad e_{t}, \quad \theta_{1} e_{t-2} \quad \quad \quad)\right.\\+& 
\operatorname{Cov}\left(\quad \quad \quad \quad e_{t}, \quad \quad \quad \quad \quad e_{t-1}\right) \end{aligned}$$

$$\begin{aligned} \operatorname{Cov}\left(Y_{t}, Y_{t-1}\right) &=\theta_{1}^{2} \operatorname{Cov}\left(e_{t-1}, e_{t-2}\right) \\ &+\theta_{1} \operatorname{Cov}\left(e_{t-1}, e_{t-1}\right) \\ &+\theta_{1} \operatorname{Cov}\left(e_{t}, e_{t-2}\right) \\ &+\quad \operatorname{Cov}\left(e_{t}, e_{t-1}\right) \end{aligned}$$

$$ \begin{aligned} \operatorname{Cov}\left(Y_{t}, Y_{t-1}\right) &=\theta_{1}^{2} \times 0 \\ &+\theta_{1} \sigma^{2} \\ &+\theta_{1} \times 0 \\ &+\quad 0  \\ &=\theta_{1} \sigma^{2} \end{aligned}$$

### Second autocovariance

$$\begin{aligned} 
\operatorname{cov}\left(Y_{t}, Y_{t-2}\right)=& \operatorname{Cov}\left(\theta_{1} e_{t-1}+e_{t}, \quad \theta_{1} e_{t-3}+e_{t-2}\right) \\=& \operatorname{Cov}\left(\theta_{1} e_{t-1} \quad \quad, \quad \theta_{1} e_{t-3} \quad \quad \quad)\right.\\+& 
\operatorname{Cov}\left(\theta_{1} e_{t-1} \quad \quad, \quad \quad \quad \quad \quad e_{t-2})\right.\\+& 
\operatorname{Cov}\left(\quad \quad \quad \quad e_{t}, \quad \theta_{1} e_{t-3} \quad \quad \quad)\right.\\+& 
\operatorname{Cov}\left(\quad \quad \quad \quad e_{t}, \quad \quad \quad \quad \quad e_{t-2}\right) \end{aligned}$$

$$\begin{aligned} \operatorname{Cov}\left(Y_{t}, Y_{t-2}\right) &=\theta_{1}^{2} \operatorname{Cov}\left(e_{t-1}, e_{t-3}\right) \\ &+
\theta_{1} \operatorname{Cov}\left(e_{t-1}, e_{t-2}\right) \\ &+
\theta_{1} \operatorname{Cov}\left(e_{t}, e_{t-3}\right) \\ &+
\quad \operatorname{Cov}\left(e_{t}, e_{t-2}\right) \end{aligned}$$

$$ \begin{aligned} \operatorname{Cov}\left(Y_{t}, Y_{t-1}\right) &=\theta_{1}^{2} \times 0 \\ &+\theta_{1} \times 0 \\ &+\theta_{1} \times 0 \\ &+\quad 0  \\ &=\theta_{1} \sigma^{2} \end{aligned}$$

### First autocorrelation

$$\begin{aligned} \gamma_{0} &= \sigma^{2}\left(1+\theta^{2}\right) \\ \gamma_{1} &= -\theta \sigma^{2} \\ \rho_{1} &= \frac{-\theta \sigma^{2}}{\sigma^{2}\left(1+\theta^{2}\right)} \\ \rho_{1} &= \frac{-\theta}{\left(1+\theta^{2}\right)}\end{aligned}$$

### Second autocorrelation

$$\begin{aligned} \rho_{2} &=\frac{\gamma_{2}}{\gamma_{0}} \\ \rho_{2} &=\frac{0}{\sigma^{2}\left(1+\theta^{2}\right)} \\ \rho_{2} &=0 \end{aligned}$$

### General expression for the autocorrelation

$$\begin{array}{c}{\gamma_{k}=0 \text { for all } k \geq 2} \\ {\rho_{k}=\frac{\gamma_{k}}{\gamma_{0}}=0 \text { for all } k \geq 2}\end{array}$$

## Derivation for MA(2)

### Model


$$ Y_{t}=e_{t}-\theta_{1} e_{t-1}-\theta_{2} e_{t-2} $$
$$  e_{t} \sim \operatorname{llD}\left(0, \sigma^{2}\right) $$

### Mean

$$\begin{array}{l}{E\left(Y_{t}\right)=E\left(e_{t}-\theta_{1} e_{t-1}-\theta_{2} e_{t-2}\right)} \\ {E\left(Y_{t}\right)=E\left(e_{t}\right)-E\left(\theta_{1} e_{t-1}\right)-E\left(\theta_{2} e_{t-2}\right)} \\ {E\left(Y_{t}\right)=E\left(e_{t}\right)-\theta_{1} E\left(e_{t-1}\right)-\theta_{2} E\left(e_{t-2}\right)} \\ {E\left(Y_{t}\right)=0-\theta_{1} \times 0-\theta_{2} \times 0} \\ {E\left(Y_{t}\right)=0}\end{array}$$

### Variance

$$\begin{array}{l}{\gamma_{0}=V\left(Y_{t}\right)} \\ {\gamma_{0}=V\left(e_{t}-\theta_{1} e_{t-1}-\theta_{2} e_{t-2}\right)} \\ {\gamma_{0}=V\left(e_{t}\right)+V\left(-\theta_{1} e_{t-1}\right)+V\left(-\theta_{2} e_{t-2}\right)} \\ {\gamma_{0}=\sigma^{2}+\theta_{1}^{2} \sigma^{2}+\theta_{2}^{2} \sigma^{2}} \\ {\gamma_{0}=\sigma^{2}\left(1+\theta_{1}^{2}+\theta_{2}^{2}\right)}\end{array}$$

### First autocovariance

$$\begin{array}{ll}{\gamma_{1}=} & {\text { Cov }\left[Y_{t},Y_{t-1}\right]} \\ {\gamma_{1}=} & {\text { Cov } \left[\left(e_{t}-\theta_{1} e_{t-1}-\theta_{2} e_{t-2}\right),\left(e_{t-1}-\theta_{1} e_{t-2}-\theta_{2} e_{t-3}\right)\right]}\end{array}$$

So all the covariances except $\operatorname{Cov}\left(-\theta_{1} e_{t-1}\right), e_{t-1}$ and $\operatorname{Cov}\left(-\theta_{2} e_{t-2},-\theta_{1} e_{t-2}\right)$ will be zero.

We have that 

$$\begin{array}{c}{\operatorname{Cov}\left(-\theta_{1} e_{t-1}, e_{t-1}\right)=-\theta_{1} \sigma^{2}} \\ {\operatorname{Cov}\left(-\theta_{2} e_{t-2},-\theta_{1} e_{t-2}\right)=\theta_{1} \theta_{2} \sigma^{2}}\end{array}$$

So

$$\begin{array}{l}{\gamma_{1}=0+0+0-\theta_{1} \sigma^{2}+0+0+\theta_{1} \theta_{2} \sigma^{2}+0} \\ {\gamma_{1}=\sigma^{2}\left(\theta_{1} \theta_{2}-\theta_{1}\right)} \\ {\gamma_{1}=\sigma^{2} \theta_{1}\left(\theta_{2}-1\right)}\end{array} $$

\newpage

### Second autocovariance

$${\gamma_{2}=\quad \text {Cov} \quad\left[Y_{t} \quad, Y_{t-2}\right]}$$
$$\begin{aligned}Y_{t} &= e_{t}-\theta_{1} e_{t-1}-\theta_{2} e_{t-2} \\ Y_{t-2} &= e_{t-2}-\theta_{1} e_{t-3}-\theta_{2} e_{t-4} \end{aligned}$$

Thus,

$$\gamma_{2}= \text{Cov}[(e_{t}-\theta_{1} e_{t-1}-\theta_{2} e_{t-2}),(e_{t-2}-\theta_{1} e_{t-3}-\theta_{2} e_{t-4})]$$
$$\gamma_{2}=0+0+0+0+0+0+\operatorname{Cov}\left(-\theta_{2} e_{t-2}, e_{t-2}\right)+0 \ldots$$
$$\begin{array}{l}{\gamma_{2}=0+0+0+0+0+0+\operatorname{Cov}\left(-\theta_{2} e_{t-2}, e_{t-2}\right)+0 \ldots} \\ {\gamma_{2} =-\theta_{2}Var\left(e_{t-2}, e_{t-2}\right)} \\ {\gamma_{2}=-\theta_{2} \sigma^{2}}\end{array}$$

### First autocorrelation

$$\rho_{k}=\frac{\gamma_{k}}{\gamma_{0}}$$

$${\gamma_{1}=\sigma^{2} \theta_{1}\left(\theta_{2}-1\right)}$$

$${\gamma_{0}=\sigma^{2}\left(1+\theta_{1}^{2}+\theta_{2}^{2}\right)}$$

thus, the first autocorrelation is

$$\rho_{1}=\frac{\gamma_{1}}{\gamma_{0}}=\frac{\theta_{1}\left(\theta_{2}-1\right)}{\left(1+\theta_{1}^{2}+\theta_{2}^{2}\right)}$$

### Second autocorrelation

$$\rho_{k}=\frac{\gamma_{k}}{\gamma_{0}}$$

$${\gamma_{2}=-\theta_{2} \sigma^{2}}$$

$${\gamma_{0}=\sigma^{2}\left(1+\theta_{1}^{2}+\theta_{2}^{2}\right)}$$


thus, the second autocorrelation is

$$\rho_{2}=\frac{\gamma_{2}}{\gamma_{0}}=\frac{-\theta_{2}}{\left(1+\theta_{1}^{2}+\theta_{2}^{2}\right)}$$

### General expression for the autocorrelation

$$\begin{array}{c}{\gamma_{k}=0 \text { for all } k \geq 3} \\ {\rho_{k}=\frac{\gamma_{k}}{\gamma_{0}}=0 \text { for all } k \geq 3}\end{array}$$


## Derivation for AR(1)

### Model

$$Y_{t}=\phi_{0}+\phi_{1} Y_{t-1}+e_{t}$$

Where we assume 

$$\operatorname{Cov}\left(Y_{t}, e_{t}\right)=0$$

and where $e_{t} \sim iid(0,1)$

### Mean

$$\begin{array}{l}{E\left(Y_{t}\right)=E\left(\phi_{0}+\phi_{1} Y_{t-1}+e_{t}\right)} \\ {E\left(Y_{t}\right)=\phi_{0}+\phi_{1} E\left(Y_{t-1}\right)+E\left(e_{t}\right)}\end{array}$$

Note that under covariance stationarity, we have the following:

$$E\left(Y_{t-1}\right)=E\left(Y_{t}\right)$$

thus, 

$$\begin{aligned} E\left(Y_{t}\right) &=\phi_{0}+\phi_{1} E\left(Y_{t}\right)+0 \\ E\left(Y_{t}\right)-\phi_{1} E\left(Y_{t}\right) &=\phi_{0} \\ E\left(Y_{t}\right)\left(1-\phi_{1}\right) &=\phi_{0} \\ E\left(Y_{t}\right) &=\frac{\phi_{0}}{\left(1-\phi_{1}\right)} \end{aligned}$$

### Variance

$$\begin{array}{l}{\gamma_{0}=Var\left(\phi_{1} Y_{t-1}+e_{t}\right)} \\ {\gamma_{0}=Var\left(\phi_{1} Y_{t-1}\right)+Var\left(e_{t}\right)+2 \operatorname{Cov}\left(\phi_{1} Y_{t-1}, e_{t}\right)} \\ {\gamma_{0}=\phi_{1}^{2} Var\left(Y_{t-1}\right)+Var\left(e_{t}\right)+2 \phi_{1} \operatorname{Cov}\left( Y_{t-1}, e_{t}\right)} \\ {\gamma_{0}=\phi_{1}^{2} Var\left(Y_{t}\right)+\sigma^{2}+2 \phi_{1} \times 0} \\ {\gamma_{0}=\phi_{1}^{2}\gamma_{0}+\sigma^{2}} \\ \gamma_0=\sigma^2/(1-\phi_1^2)\end{array}$$

### First autocovariance

$$\begin{array}{l}{\gamma_{1}=Cov\left(Y_{t},Y_{t-1}\right)} \\ {\gamma_{1}=Cov\left(\phi Y_{t-1}+e_{t},Y_{t-1}\right)} \\ {\gamma_{1}=Cov\left(\phi Y_{t-1},Y_{t-1}) + Cov(e_{t},Y_{t-1}\right)} \\ {\gamma_{1}=Cov\left(\phi Y_{t-1},Y_{t-1}\right) + 0} \\ {\gamma_{1}=\phi Cov(Y_{t-1},Y_{t-1})} \\ \end{array}$$
Since we assume stationarity, the autocovariance only depends on the lag and not on where in the process we are. Hence, the covariance does not change when we lead or lag the variables, as long as we make sure to keep the distance between them. We  lead the variables in the expression above, and get;
$${\gamma_{1}=\phi Cov(Y_{t},Y_{t-0})}$$
and since
$${Cov(Y_{t},Y_{t-0}) = Var(Y_{t-0}) = \gamma_{0}}$$

we have

$$\gamma_{1}=\phi \gamma_{0}$$
By inserting the expression for $\gamma_0$, we get;
$$\gamma_1= \phi*\sigma^2 / (1-\phi^2)$$


### Second autocovariance

$$\begin{array}{l}{\gamma_{2}=Cov\left(Y_{t},Y_{t-2}\right)} \\ {\gamma_{2}=Cov\left(\phi Y_{t-1}+e_{t},Y_{t-2}\right)} \\ {\gamma_{2}=Cov\left(\phi Y_{t-1},Y_{t-2}\right) = 0} \\ {\gamma_{2}=\phi Cov\left(Y_{t-1},Y_{t-2}\right)}\end{array}$$
Assuming stationarity, we can lead the variables and get;
$${\gamma_{2}=\phi Cov\left(Y_{t},Y_{t-1}\right)}$$
Since
$${Cov(Y_{t},Y_{t-1}) = Var(Y_{t-1}) = \gamma_{1}}$$

We have;

$$\gamma_{2}=\phi \gamma_{1}$$

By inserting the expression for $\gamma_1$, we get;
$$\gamma_2= \Phi^2*\sigma^2 / (1-\Phi^2)$$


### First autocorrelation

$$\rho_{k}=\phi^k$$

thus we have

$$\rho_{1}=\phi^1=\phi$$

### Second autocorrelation

$$\rho_{k}=\phi^k$$

thus we have

$$\rho_{2}=\phi^2$$

## Derivation for AR(2)

### Model
We have the general AR(2) model:
$$Y_{t}=\phi_{0}+\phi_{1} Y_{t-1}+\phi_{2} Y_{t-2}+e_{t}$$

Where we assume 

$$\operatorname{Cov}\left(Y_{t-1}, e_{t}\right)=0$$

and

$$\operatorname{Cov}\left(Y_{t-2}, e_{t}\right)=0$$

and where $e_{t} \sim iid(0,1)$


### Mean

$$\begin{array}{l}{E\left(Y_{t}\right)=E\left(\phi_{0}+\phi_{1} Y_{t-1}+\phi_{2} Y_{t-2}+e_{t}\right)} \\ {E\left(Y_{t}\right)=\phi_{0}+\phi_{1} E\left(Y_{t-1}\right)+\phi_{2} E(Y_{t-2})+E\left(e_{t}\right)}\\ E\left(Y_{t}\right)=\phi_{0}+\phi_{1} E\left(Y_{t-1}\right)+\phi_{2} E(Y_{t-2}) \end{array}$$

then we assume stationarity, so that the expected value is constant over time;

$$\begin{array}{l}{E\left(Y_{t}\right)-\phi_{1} E\left(Y_{t}\right)+\phi_{2} E(Y_{t}) = \phi_{0}} \\
{E\left(Y_{t}\right)(1-\phi_{1} -\phi_{2}) = \phi_{0}} \end{array}$$

thus

$$\begin{array}{l}{E\left(Y_{t}\right)=\phi_{0}/(1 - \phi_{1}- \phi_{2})} \end{array}$$


### Variance
$$\begin{array}{l}{\gamma_{0}=Cov\left(Y_{t} , Y_{t}\right)} \\ {\gamma_{0}=Cov\left(\phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + e_{t} , Y_{t}\right)} \\ {\gamma_{0}=Cov\left(\phi_{1} Y_{t-1}, Y_{t}\right)}+ {Cov\left(\phi_{2} Y_{t-2}, Y_{t}\right)} + {Cov\left(Y_{t}, e_{t}\right)} \\ {\gamma_{0}=Cov\left(\phi_{1} Y_{t-1}, Y_{t}\right)}+{Cov\left(\phi_{2} Y_{t-2}, Y_{t}\right)}+\sigma^2\\{\gamma_{0}=\phi_{1}Cov\left(Y_{t-1}, Y_{t}\right)}+{\phi_{2}Cov\left( Y_{t-2}, Y_{t}\right)}+\sigma^2 \end{array}$$
Since we assume stationairity, we can lead the variables;
$${\gamma_{0}=\phi_{1}Cov\left(Y_{t}, Y_{t+1}\right)}+{\phi_{2}Cov\left( Y_{t}, Y_{t+2}\right)}+\sigma^2 \\{\gamma_{0}=\phi_{1} \gamma_{-1}+\phi_{2}\gamma_{-2}+\sigma^2}$$
Which of course, under stationarity, also can be expressed as;
$$\gamma_{0}=\phi_{1} \gamma_{1}+\phi_{2}\gamma_{2}+\sigma^2$$

### The First autocovariance
$$\begin{array}{l}{\gamma_{1}=Cov\left(Y_{t} , Y_{t-1}\right)} \\ {\gamma_{1}=Cov\left(\phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + e_{t} , Y_{t-1}\right)}\\ {\gamma_{1}=Cov\left(\phi_{1} Y_{t-1}, Y_{t-1}\right)} + {Cov\left(\phi_{2} Y_{t-2}, Y_{t-1}\right)} + {Cov\left(e_{t}, Y_{t-1}\right)} \\ {\gamma_{1}=Cov\left(\phi_{1} Y_{t-1}, Y_{t-1}\right)} + {Cov\left(\phi_{2} Y_{t-2}, Y_{t-1}\right)+0}\\{\gamma_{1}=\phi_{1} Cov\left(Y_{t-1}, Y_{t-1}\right)} + {\phi_{2}Cov\left(Y_{t-2}, Y_{t-1}\right)}\end{array}$$
Assuming stationarity, we can lead the variables as many times as we want, as long as we keep the distance between them constant:
$${\gamma_{1}=\phi_{1} Cov\left(Y_{t}, Y_{t}\right)} + {\phi_{2}Cov\left(Y_{t}, Y_{t+1}\right)}$$
$$\gamma_{1}=\phi_{1}\gamma_{0}+\phi_{2}\gamma_{-1}$$
Which then of course also can be expressed as:
$$\gamma_{1}=\phi_{1}\gamma_{0}+\phi_{2}\gamma_{1}$$

### The second autocovariance
$$\begin{array}{l}{\gamma_{2}=Cov\left(Y_{t} , Y_{t-2}\right)} \\ {\gamma_{2}=Cov\left(\phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + e_{t} , Y_{t-2}\right)}\\ {\gamma_{2}=Cov\left(\phi_{1} Y_{t-1}, Y_{t-2}\right)} + {Cov\left(\phi_{2} Y_{t-2}, Y_{t-2}\right)}+ {Cov\left(e_{t}, Y_{t-2}\right)} \\ {\gamma_{2}=Cov\left(\phi_{1} Y_{t-1}, Y_{t-2}\right)} + {Cov\left(\phi_{2} Y_{t-2}, Y_{t-2}\right)+0}\\{\gamma_{2}=\phi_{1} (Cov\left(Y_{t-1}, Y_{t-2}\right)} + {\phi_{2}Cov\left(Y_{t-2}, Y_{t-2}\right)}\end{array}$$
Assuming stationarity, we can lead the variables as many times we wish, as long as we keep the distance between them;
$${\gamma_{2}=\phi_{1} (Cov\left(Y_{t}, Y_{t-1}\right)} + {\phi_{2}Cov\left(Y_{t}, Y_{t}\right) \\ \gamma_{2}=\phi_{1}\gamma_{1}+\phi_{2}\gamma_{0}}$$



### The Equation system for the variance and autocovariances
The expressions for $\gamma_0$,$\gamma_1$ and $\gamma_2$ provide an equation system with three equations and three unknowns:
$$(i) \ \ \ \ \gamma_0=\phi_1\gamma_1+\phi_2\gamma_{2}+\sigma^2 \\(ii) \ \ \ \ \ \ \ \ \  \gamma_1=\phi_{1}\gamma_{0}/(1-\phi_{2}) \\ (iii) \ \ \ \ \ \ \ \ \ \ \  \gamma_{2}=\phi_{1}\gamma_{1}+\phi_{2}\gamma_{0}$$
This equation system has to be solved in order to get the variance and the autocovariances as functions of $\phi$ and $\sigma^2$.
(i) $$ => \gamma_2 = (\gamma_0-\phi_1\gamma_1-\sigma^2) / \phi_2$$
Insert (ii) in (iii) => $$\gamma_2=(\phi_1^2\gamma_0) / (1-\theta_2) + \phi_2\gamma_0$$
Equating the both expressions:
$$\gamma_2=\gamma_2 \\ (\gamma_0-\phi_1\gamma_1-\sigma^2) / \phi_2 = (\phi_1^2\gamma_0) / (1-\theta_2) + \phi_2\gamma_0$$
Solving for $\gamma_0$ gives;
$$\gamma_0 = \sigma^2(1-\phi_2)  /  (1-\phi_2-\phi_1^2-\phi_1^2\phi_2-\phi_2^2-\phi_2^3)$$
Inserting this in (ii) gives an expression for $\gamma_1$:

$$\gamma_1 = \phi_1\sigma^2  /  [(1-\phi_2)(1-\phi_2-\phi_1^2-\phi_1^2\phi_2-\phi_2^2-\phi_2^3)]$$
And inserting the expressions for $\gamma_0$ and $\gamma_1$ into (iii) gives the expression for $\gamma_2$:
$$\gamma_2 = [\sigma^2(\phi_1^2+\phi_2(1-\phi_2)^2)]  /  [(1-\phi_2)(1-\phi_2-\phi_1^2-\phi_1^2\phi_2-\phi_2^2-\phi_2^3)]$$








### First autocorrelation

$$\rho_{k}=\phi_{1}(\gamma_{k-1}/\gamma_{0})+\phi_{2}(\gamma_{k-2}/\gamma_{0}) $$

which is

$$\rho_{k}=\phi_{1}\rho_{k-1}+\phi_{2}\rho_{k-2} $$

Correlations are symmetrical, so

$$\rho_{k}=\phi_{1}\rho_{1}+\phi_{2}\rho_{2}$$

$$\rho_{2}=\phi_{1}\rho_{0}+\phi_{0}\rho_{1}$$

meaning that

$$\rho_{1}=(\phi_{1}/(1-\phi_{2}))\rho_{0}$$

$$\rho_{0} = \gamma_{0}/\gamma_{0} = 1$$

so the first autocorrelation is

$$\rho_{1}=(\phi_{1}/(1-\phi_{2}))$$

### Second autocorrelation

$$\rho_{k}=\phi_{1}(\gamma_{k-1}/\gamma_{0})+\phi_{2}(\gamma_{k-2}/\gamma_{0}) $$

which is

$$\rho_{k}=\phi_{1}\rho_{k-1}+\phi_{2}\rho_{k-2} $$

Correlations are symmetrical, so

$$\rho_{k}=\phi_{1}\rho_{1}+\phi_{2}\rho_{2}$$

thus

$$\rho_{1}=\phi_{1}\rho_{1}+\phi_{2}\rho_{0}$$

meaning that

$$\rho_{1}=\phi_{1}\rho_{1}+\phi_{2}$$
since

$$\rho_{0} = \gamma_{0}/\gamma_{0} = 1$$

so the second autocorrelation is

$$\rho_{2}=\phi_{1}(\phi_{1}/(1-\phi_{2}))+\phi_{2}$$

which is the same as

$$\rho_{2}=\phi^2_{1}/(1-\phi_{2})+\phi_{2}$$
we convert them to one numenator and get

$$\rho_{2}=\phi^2_{1}+\phi_{2}(1-\phi_{2})/(1-\phi_{2})$$

MA(1), $\theta = 1$, table 1, appendix B


## Derivation for ARMA(1,1)

ARMA(1,1): 

$$Y_t = \Phi Y_{t-1} + e_t - \theta e_{t-1}$$

$$e_t \sim NID(0, 1)$$

### Mean

$$Y_t = \Phi Y_{t-1} + e_t - \theta e_{t-1}$$

$$\begin{aligned} E\left(Y_{t}\right)
&=E\left(\phi Y_{t-1}+e_{t}+\theta e_{t-1}\right) \\ 
&=\phi E\left(Y_{t-1}\right)+E\left(e_{t}\right)+\theta E\left(e_{t-1}\right) \\ 
&=\phi E\left(Y_{t}\right) \end{aligned}$$

since $E\left(Y_{t-1}\right) = E\left(Y_{t-j}\right)$ for all j (stationarity property)

$$E(Y_t) = \frac{1}{1-\phi}$$

### Variance and autocovariance

Note that:

$$ \begin{aligned}\phi(B) Y_{t} &= \theta(B) e_{t} \\ [\phi(B)]^{1} Y_{t} &= \theta(B) e_{t} \\ Y_{t} &= [\phi(B)]^{-1} \theta(B) e_{t} \end{aligned}$$

That is:

$$Y_{t}=\frac{\theta(B)}{\phi(B)} e_{t}$$

Let:

$$ \frac{\theta(B)}{\phi(B)}=\Sigma_{j=0}^{\infty} \psi_{j} B^{j} $$

where $\phi_j$ would be a function of the "original parameters", that is:

$$\psi_{j}=f(\theta, \phi)$$

Thus:

$$ \begin{array}{l}{Y_{t}=\left[\frac{\theta(B)}{\phi(B)}\right] e_{t}} \\ {Y_{t}=\left[\Sigma_{j=0}^{\infty} \psi_{j} B^{j}\right] e_{t}} \\ {Y_{t}=\psi_{0} B^{0} e_{t}+\psi_{1} B^{1} e_{t}+\psi_{2} B^{2} e_{t}+\psi_{3} B^{3} e_{t}+\psi_{4} B^{4} e_{t}+\ldots} \\ {Y_{t}=\psi_{0} e_{t-0}+\psi_{1} e_{t-1}+\psi_{2} e_{t-2}+\psi_{3} e_{t-3}+\psi_{4} e_{t-4}+\ldots}\end{array} $$

which is a general linear process. Proceed in the same way and note that the Yule-Walker equations are the same as those of an AR(1) for k > 1.

**For k = 0:**

To simplify, let $m = E(X_t)$

$$\begin{aligned} \gamma_{0} &=E\left[\left(Y_{t}-m\right)\left(Y_{t}-m\right)\right] \\ &=E\left[\left(\phi\left(Y_{t-1}-m\right)+e_{t}+\theta e_{t-1}\right)\left(Y_{t}-m\right)\right] \\ 
&=\phi E\left[\left(Y_{t}-m\right)\left(Y_{t-1}-m\right)\right]+E\left[e_{t}\left(Y_{t}-m\right)\right]+\theta \underbrace{E\left[e_{t-1}\left(Y_{t}-m\right)\right]}_{\neq 0} \\ 
&=\underbrace{\phi \gamma_{1}+\sigma^{2}}_{AR(1) \text { part }} +\theta E\left[\left(\phi\left(Y_{t-1}-m\right)+e_{t}+\theta e_{t-1}\right) e_{t-1}\right] \\ 
&=\phi \gamma_{1}+\sigma^{2}+\theta \phi E\left[\left(Y_{t-1}-m\right) e_{t-1}\right]+\theta E\left[e_{t} e_{t-1}\right]+\theta^{2} E\left[e_{t-1}^{2}\right] \\ 
&=\phi \gamma_{1}+\sigma^{2}(1+\theta(\phi+\theta)) \end{aligned}$$

**For k = 1:**

$$ \begin{aligned} \gamma_{1}=& E\left[\left(Y_{t}-m\right)\left(Y_{t-1}-m\right)\right] \\=& E\left[\left(\phi\left(Y_{t-1}-m\right)+e_{t}+\theta e_{t-1}\right)\left(Y_{t-1}-m\right)\right] \\=& \phi E\left[\left(Y_{t-1}-m\right)\left(Y_{t-1}-m\right)\right]+E\left[e_{t}\left(Y_{t-1}-m\right)\right] \\ &+\theta E\left[e_{t-1}\left(Y_{t-1}-m\right)\right] \\=& \underbrace{\phi \gamma_{0}}_{\mathrm{AR}(1) \text { part }}+\theta \sigma^{2} \end{aligned} $$

Solving for $\gamma_0$ and $\gamma_1$ :

$$\begin{aligned} \gamma_{0} & \equiv V\left(Y_{t}\right)=\frac{1+2 \phi \theta+\theta^{2}}{1-\phi^{2}} \sigma^{2} \\ 
\gamma_{1} & \equiv \operatorname{Cov}\left(Y_{t}, Y_{t-1}\right)=\frac{(\phi+\theta)(1+\phi \theta)}{1-\phi^{2}} \sigma^{2} \end{aligned}$$

**For k > 1:**

$$\begin{aligned} \gamma_{k}=& E\left[\left(Y_{t}-m\right)\left(Y_{t-k}-m\right)\right] \\=& E\left[\left(\phi\left(Y_{t-1}-m\right)+e_{t}+\theta e_{t-1}\right)\left(Y_{t-k}-m\right)\right] \\=& \phi E\left[\left(Y_{t-1}-m\right)\left(Y_{t-k}-m\right)\right]+E\left[e_{t}\left(Y_{t-k}-m\right)\right] \\ +& \theta E\left[e_{t-1}\left(Y_{t-k}-m\right)\right]+E\left[e_{t}\left(Y_{t-k}-m\right)\right] \\ =& \underbrace{\phi \gamma_{k-1}}_{\text {AR(1) part }} \end{aligned}$$

### Correlation

$$\rho_{k}=\left\{\begin{array}{l}{1 \text { if } k=0} \\ {\frac{(\phi+\theta)(1+\phi \theta)}{1+2 \phi \theta+\theta^{2}} \text { if }|k|=1} \\ {\phi \rho_{k-1} \text { if }|k|>1}\end{array}\right.$$

\newpage

# Appendix B

## Models - MA(1)

\begin{table}[H]
  \centering
  \begin{tabular}{ | l | c | }
    \hline
    \diagbox[]{$\theta{_1}$}{$\theta{_2}$} & $\theta{_2} = 0$ \\ 
    \hline
    $\theta{_1} = -1.0$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/1}
    \end{minipage} \\
    \hline
    $\theta{_1} = -0.45$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/2}
    \end{minipage} \\
    \hline
    $\theta{_1} = 0$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/3}
    \end{minipage} \\
    \\ 
    \hline
  \end{tabular}
  \caption{Table of MA(1) - (1 of 2)}
\end{table}

\newpage

\begin{table}[H]
  \centering
  \begin{tabular}{ | l | c | }
    \hline
    \diagbox[]{$\theta{_1}$}{$\theta{_2}$} & $\theta{_2} = 0$ \\
    \hline
    $\theta{_1}= 0.45$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/4}
    \end{minipage} \\
    \\ 
    \hline
    $\theta{_1} = 1$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/5}
    \end{minipage} \\
    \\ 
    \hline
    $\theta{_1} = 2$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/6}
    \end{minipage} \\
    \\ 
    \hline
  \end{tabular}
  \caption{Table of MA(1) - (2 of 2)}
\end{table}

\newpage

## Models - MA(2)

\begin{table}[H]
  \centering
  \begin{tabular}{ | l | c | }
    \hline
    \diagbox[]{$\theta{_1}$}{$\theta{_2}$} & $\theta{_2} = 0$ \\
    \hline
    $\theta{_1} = -0.8$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/7}
    \end{minipage} \\
    \hline
    $\theta{_1} = 0$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/10}
    \end{minipage} \\
    \hline
    $\theta{_1} = 0.8$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/13}
    \end{minipage} \\
    \\ 
    \hline
  \end{tabular}
  \caption{Table of MA(2) - (1 of 3)}
\end{table}

\newpage

\begin{table}[H]
  \centering
  \begin{tabular}{ | l | c | }
    \hline
    \diagbox[]{$\theta{_1}$}{$\theta{_2}$} & $\theta{_2} = 0.7$ \\
    \hline
    $\theta{_1} = -0.8$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/8}
    \end{minipage} \\
    \\ 
    \hline
    $\theta{_1} = 0$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/11}
    \end{minipage} \\
    \\ 
    \hline
    $\theta{_1} = 0.8$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/14}
    \end{minipage} \\
    \\ 
    \hline
  \end{tabular}
  \caption{Table of MA(2) - (2 of 3)}
\end{table}

\newpage

\begin{table}[H]
  \centering
  \begin{tabular}{ | l | c | }
    \hline
    \diagbox[]{$\theta{_1}$}{$\theta{_2}$} & $\theta{_2} = 1$ \\ 
    \hline
    $\theta{_1} = -0.8$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/9}
    \end{minipage} \\
    \\ 
    \hline
    $\theta{_1} = 0$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/12}
    \end{minipage} \\
    \\ 
    \hline
    $\theta{_1} = 0.8$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/15}
    \end{minipage} \\
    \\ 
    \hline
  \end{tabular}
  \caption{Table of MA(2) - (3 of 3)}
\end{table}

\newpage

## Models - AR(1)

\begin{table}[H]
  \centering
  \begin{tabular}{ | l | c | }
    \hline
    \diagbox[]{$\phi{_1}$}{$\phi{_2}$} & $\phi{_2} = 0$ \\ 
    \hline
    $\phi{_1} = -0.1$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/16}
    \end{minipage} \\
    \\ 
    \hline
    $\phi{_1} = -0.95$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/17}
    \end{minipage} \\
    \\ 
    \hline
    $\phi{_1} = -0.75$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/18}
    \end{minipage} \\
    \\ 
    \hline
  \end{tabular}
  \caption{Table of AR(1) - (1 of 2)}
\end{table}

\newpage

\begin{table}[H]
  \centering
  \begin{tabular}{ | l | c | }
    \hline
    \diagbox[]{$\phi{_1}$}{$\phi{_2}$} & $\phi{_2} = 0$ \\ 
    \hline
    $\phi{_1} = 0$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/19}
    \end{minipage} \\
    \\ 
    \hline
    $\phi{_1} = 0.75$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/20}
    \end{minipage} \\
    \\ 
    \hline
    $\phi{_1} = 0.95$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/21}
    \end{minipage} \\
    \\ 
    \hline
    $\phi{_1} = 1$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/22}
    \end{minipage} \\
    \\ 
    \hline
  \end{tabular}
  \caption{Table of AR(1) - (2 of 2)}
\end{table}

\newpage

## Models - AR(2)

\begin{table}[H]
  \centering
  \begin{tabular}{ | l | c | }
    \hline
    \diagbox[]{$\phi{_1}$}{$\phi{_2}$} & $\phi{_2} = 0.1$ \\ 
    \hline
    $\phi{_1} = -0.9$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/23}
    \end{minipage} \\
    \\ 
    \hline
    $\phi{_1} = 0$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/26}
    \end{minipage} \\
    \\ 
    \hline
    $\phi{_1} = 0.7$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/28}
    \end{minipage} \\
    \\ 
    \hline
  \end{tabular}
  \caption{Table of AR(2) - (1 of 3)}
\end{table}

\newpage

\begin{table}[H]
  \centering
  \begin{tabular}{ | l | c | }
    \hline
    \diagbox[]{$\phi{_1}$}{$\phi{_2}$} & $\phi{_2} = 0.2$ \\ 
    \hline
    $\phi{_1} = -0.9$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/24}
    \end{minipage} \\
    \\ 
    \hline
    $\phi{_1} = 0$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/27}
    \end{minipage} \\
    \\ 
    \hline
    $\phi{_1} = 0.7$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/30}
    \end{minipage} \\
    \\ 
    \hline
  \end{tabular}
  \caption{Table of AR(2) - (2 of 3)}
\end{table}

\newpage

\begin{table}[H]
  \centering
  \begin{tabular}{ | l | c | }
    \hline
    \diagbox[]{$\phi{_1}$}{$\phi{_2}$} & $\phi{_2} = 0.8$ \\ 
    \hline
    $\phi{_1} = -0.9$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/25}
    \end{minipage} \\
    \\ 
    \hline
    $\phi{_1} = 0$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/28}
    \end{minipage} \\
    \\ 
    \hline
    $\phi{_1} = 0.7$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/31}
    \end{minipage} \\
    \\ 
    \hline
  \end{tabular}
  \caption{Table of AR(3) - (3 of 3)}
\end{table}

\newpage

## Models - ARMA(1,1)

\begin{table}[H]
  \centering
  \begin{tabular}{ | l | c | }
    \hline
    \diagbox[]{$\phi$}{$\theta$} & $\theta = -0.4$ \\ 
    \hline
    $\phi{_1} = -0.9$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/32}
    \end{minipage} \\
    \\ 
    \hline
    $\phi{_1} = 0.8$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/34}
    \end{minipage} \\
    \\ 
    \hline
    $\phi{_1} = 0.9$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/36}
    \end{minipage} \\
    \\ 
    \hline
  \end{tabular}
  \caption{Table of ARMA(1,1) - (1 of 2)}
\end{table}

\newpage

\begin{table}[H]
  \centering
  \begin{tabular}{ | l | c | }
    \hline
    \diagbox[]{$\phi$}{$\theta$} & $0.4$ \\ 
    \hline
    $-0.9$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/33}
    \end{minipage} \\
    \\ 
    \hline
    $0.8$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/35}
    \end{minipage} \\
    \\ 
    \hline
    $0.9$
    &
    \begin{minipage}{15cm}
      \includegraphics[width=\linewidth, height=500mm]{figure/37}
    \end{minipage} \\
    \\ 
    \hline
  \end{tabular}
  \caption{Table of ARMA(1,1) - (2 of 2)}
\end{table}